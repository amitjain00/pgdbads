{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8dfb42d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amit.Jain\\Downloads\\part-00000-0c827b90-3d7d-4fea-8fae-6e77c3bcd63a-c000.snappy.parquet\n",
      "+--------------------+\n",
      "|             payload|\n",
      "+--------------------+\n",
      "|{\"Id\":20,\"Ncpdp\":...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyFirstProgram\").getOrCreate()\n",
    "text_file = \"C:\\\\Users\\\\Amit.Jain\\\\Downloads\\\\learn_Spark\\\\ml-100k\\\\u.data\"\n",
    "inputData_Text = spark.read.text(text_file)\n",
    "parquet_file = \"C:\\\\Users\\\\Amit.Jain\\\\Downloads\\\\part-00000-0c827b90-3d7d-4fea-8fae-6e77c3bcd63a-c000.snappy.parquet\"\n",
    "print(parquet_file)\n",
    "Input_parquet = spark.read.parquet(parquet_file)\n",
    "Input_parquet.createOrReplaceTempView(\"pharmacy\")\n",
    "\n",
    "pharmacy_payload = spark.sql(\"select * from pharmacy\")\n",
    "pharmacy_payload.take(5)\n",
    "#myResultDataFrame.select(\"aggregateId\",\"aggregateType\", \"eventType\" , \"payload\" ).show()\n",
    "payLoads = pharmacy_payload.select(\"payload\" )\n",
    "payLoads.select(\"payload\" ).show()\n",
    "jsondata = spark.read.json(payLoads.rdd.map(lambda row: row.payload))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2255e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amit.Jain\\Downloads\\pharmacy.json\n",
      "+---------+-----------+------------+------------+------------------+----------+----------------------------+----------------------+-------------------------+-------------------------+---------------------------+---------------------+--------------------+--------------+--------+--------------------+---------------------+---------------+----+-------+---------------------+---------------+--------------+-----------------+----------+----+--------------------+--------------------------+--------------------+-------------------------+-------------------+-------------+--------------+--------------------+-------+---------+----------+-------------------+------------+-----+--------------+---------------------+-------+----------------------+-----------------+----------+--------------+----------+----+---------+----+-----------------------------+-------------------+-------+---------------+-----------------------+----------------+-------------+-------------+---------------+\n",
      "|AbcNumber|AddressCity|AddressLine2|AddressState|     AddressStreet|AddressZip|AuthorizedForFdbPriceUpdates|AuthorizedForPowerLine|AuthorizedForPowerLinePpe|AuthorizedForPriceUpdates|AuthorizedForRelayHealthPpe|AuthorizedForStrandRx|BillingAccountNumber|ChangeCostBase|Contacts|             Created|CreatedByUserFullName|CreatedByUserId| Dea|Deleted|DeletedByUserFullName|DeletedByUserId|EmailAddresses|ErxPpeCardinalPae|FormerName|  Id|         Identifiers|IsEnrolledInElevateProgram|         LastUpdated|LastUpdatedByUserFullName|LastUpdatedByUserId|LocationCount|MoveGroupToPcn|                Name|  Ncpdp|NetrxType|NpiNumbers|PerformIdCrosscheck|PharmacyType|Phone|PmpContactName|PmpStateLicenseNumber|PmsType|PowerlinePpeChangeCost|PowerlinePpePrice|PpePayTier|PrePostRouteId|PrismGroup|Ptrk|PtrkGroup| Rak|RelayHealthDataCollectionOnly|RelayHealthEVoucher|SendNpi|SmithCustomerId|SurescriptsEprescribing|SwitchingRouteId|UpdateSdcCost|VerispanGroup|VoucherOnDemand|\n",
      "+---------+-----------+------------+------------+------------------+----------+----------------------------+----------------------+-------------------------+-------------------------+---------------------------+---------------------+--------------------+--------------+--------+--------------------+---------------------+---------------+----+-------+---------------------+---------------+--------------+-----------------+----------+----+--------------------+--------------------------+--------------------+-------------------------+-------------------+-------------+--------------+--------------------+-------+---------+----------+-------------------+------------+-----+--------------+---------------------+-------+----------------------+-----------------+----------+--------------+----------+----+---------+----+-----------------------------+-------------------+-------+---------------+-----------------------+----------------+-------------+-------------+---------------+\n",
      "|     null|SPARTANBURG|        null|          SC|8045 HOWARD STREET|     29303|                       false|                 false|                    false|                    false|                      false|                false|                null|         false|      []|2005-12-09T00:00:00Z|      DATA CONVERSION|           null|null|   null|                 null|           null|            []|            false|      null|9690|[{861965224524226...|                     false|2021-07-06T13:42:...|           CPE Admin User| 169611148495167488|            0|         false|TEST NABP FOR FAC...|0000238|     null|        []|              false|           1| null|          null|                 null|      0|                 false|             null|      null|          null|      null|null|     null|null|                        false|              false|  false|           null|                  false|            null|        false|         null|          false|\n",
      "+---------+-----------+------------+------------+------------------+----------+----------------------------+----------------------+-------------------------+-------------------------+---------------------------+---------------------+--------------------+--------------+--------+--------------------+---------------------+---------------+----+-------+---------------------+---------------+--------------+-----------------+----------+----+--------------------+--------------------------+--------------------+-------------------------+-------------------+-------------+--------------+--------------------+-------+---------+----------+-------------------+------------+-----+--------------+---------------------+-------+----------------------+-----------------+----------+--------------+----------+----+---------+----+-----------------------------+-------------------+-------+---------------+-----------------------+----------------+-------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyFirstProgram\").getOrCreate()\n",
    "json_file = \"C:\\\\Users\\\\Amit.Jain\\\\Downloads\\\\pharmacy.json\"\n",
    "print(json_file)\n",
    "Input_json = spark.read.option(\"multiline\",\"true\").json(json_file)\n",
    "Input_json.createOrReplaceTempView(\"Amit_json\")\n",
    "myResultDataFrame2 = spark.sql(\"select * from Amit_json\")\n",
    "myResultDataFrame2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ee0dbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amit.Jain\\Downloads\\learn_spark\\fakefriends.csv\n",
      "+---+--------+---+---+\n",
      "|_c0|     _c1|_c2|_c3|\n",
      "+---+--------+---+---+\n",
      "|  0|    Will| 33|385|\n",
      "|  1|Jean-Luc| 26|  2|\n",
      "|  2|    Hugh| 55|221|\n",
      "|  3|  Deanna| 40|465|\n",
      "|  4|   Quark| 68| 21|\n",
      "|  5|  Weyoun| 59|318|\n",
      "|  6|  Gowron| 37|220|\n",
      "|  7|    Will| 54|307|\n",
      "|  8|  Jadzia| 38|380|\n",
      "|  9|    Hugh| 27|181|\n",
      "| 10|     Odo| 53|191|\n",
      "| 11|     Ben| 57|372|\n",
      "| 12|   Keiko| 54|253|\n",
      "| 13|Jean-Luc| 56|444|\n",
      "| 14|    Hugh| 43| 49|\n",
      "| 15|     Rom| 36| 49|\n",
      "| 16|  Weyoun| 22|323|\n",
      "| 17|     Odo| 35| 13|\n",
      "| 18|Jean-Luc| 45|455|\n",
      "| 19|  Geordi| 60|246|\n",
      "+---+--------+---+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+\n",
      "|     _c1|\n",
      "+--------+\n",
      "|    Will|\n",
      "|Jean-Luc|\n",
      "|    Hugh|\n",
      "|  Deanna|\n",
      "|   Quark|\n",
      "|  Weyoun|\n",
      "|  Gowron|\n",
      "|    Will|\n",
      "|  Jadzia|\n",
      "|    Hugh|\n",
      "|     Odo|\n",
      "|     Ben|\n",
      "|   Keiko|\n",
      "|Jean-Luc|\n",
      "|    Hugh|\n",
      "|     Rom|\n",
      "|  Weyoun|\n",
      "|     Odo|\n",
      "|Jean-Luc|\n",
      "|  Geordi|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"workOnFakeFriends_csv\").getOrCreate()\n",
    "csv_file = \"C:\\\\Users\\\\Amit.Jain\\\\Downloads\\\\learn_spark\\\\fakefriends.csv\"\n",
    "print(csv_file)\n",
    "Input_csv = spark.read.csv(csv_file)\n",
    "Input_csv.createOrReplaceTempView(\"Amit_csv\")\n",
    "myResultDataFrame = spark.sql(\"select * from Amit_csv\")\n",
    "myResultDataFrame.show()\n",
    "myResultDataFrame.select(\"_c0\")\n",
    "myResultDataFrame.select(\"_c1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "831cb6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amit.Jain\\Downloads\\learn_spark\\fakefriends.csv\n",
      "Column<'ID'>\n",
      "Column<'Name'>\n",
      "Column<'age'>\n",
      "Column<'numFriends'>\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 18|    8|\n",
      "| 19|   11|\n",
      "| 20|    5|\n",
      "| 21|    8|\n",
      "| 22|    7|\n",
      "| 23|   10|\n",
      "| 24|    5|\n",
      "| 25|   11|\n",
      "| 26|   17|\n",
      "| 27|    8|\n",
      "| 28|   10|\n",
      "| 29|   12|\n",
      "| 30|   11|\n",
      "| 31|    8|\n",
      "| 32|   11|\n",
      "| 33|   12|\n",
      "| 34|    6|\n",
      "| 35|    8|\n",
      "| 36|   10|\n",
      "| 37|    9|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"workOnFakeFriends_csv\").getOrCreate()\n",
    "csv_file = \"C:\\\\Users\\\\Amit.Jain\\\\Downloads\\\\learn_spark\\\\fakefriends.csv\"\n",
    "print(csv_file)\n",
    "\n",
    "def mapper(line):\n",
    "    fields=line.split(\",\")\n",
    "    return Row(ID=int(fields[0]), Name = str(fields[1].encode(\"utf-8\")), age = int(fields[2]), numFriends = int(fields[3]))\n",
    "\n",
    "line = spark.sparkContext.textFile(csv_file)\n",
    "person = line.map(mapper)\n",
    "\n",
    "schemaPerson = spark.createDataFrame(person).cache()\n",
    "schemaPerson.createOrReplaceTempView(\"person\")\n",
    "teenager= spark.sql(\"select * from person\")\n",
    "\n",
    "for teen in teenager:\n",
    "    print(teen)\n",
    "\n",
    "schemaPerson.groupBy(\"age\").count().orderBy(\"age\").show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f90352f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|age|AverageFreinds|\n",
      "+---+--------------+\n",
      "| 18|        343.38|\n",
      "| 19|        213.27|\n",
      "| 20|         165.0|\n",
      "| 21|        350.88|\n",
      "| 22|        206.43|\n",
      "| 23|         246.3|\n",
      "| 24|         233.8|\n",
      "| 25|        197.45|\n",
      "| 26|        242.06|\n",
      "| 27|        228.13|\n",
      "| 28|         209.1|\n",
      "| 29|        215.92|\n",
      "| 30|        235.82|\n",
      "| 31|        267.25|\n",
      "| 32|        207.91|\n",
      "| 33|        325.33|\n",
      "| 34|         245.5|\n",
      "| 35|        211.63|\n",
      "| 36|         246.6|\n",
      "| 37|        249.33|\n",
      "+---+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as func\n",
    "\n",
    "spark = SparkSession.builder.appName(\"friendsByAge\").getOrCreate()\n",
    "\n",
    "csv_file = \"C:\\\\Users\\\\Amit.Jain\\\\Downloads\\\\learn_spark\\\\fakefriends-header.csv\"\n",
    "\n",
    "lines = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(csv_file)\n",
    "#lines.show()\n",
    "\n",
    "friendsByAge = lines.select(\"age\", \"friends\")\n",
    "friendsByAge.groupBy(\"age\").agg(func.round(func.avg(\"friends\"), 2).alias(\"AverageFreinds\")).sort(\"age\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "24d4499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|  stationID|min(temperature)|\n",
      "+-----------+----------------+\n",
      "|ITE00100554|          -148.0|\n",
      "|EZE00100082|          -135.0|\n",
      "+-----------+----------------+\n",
      "\n",
      "+-----------+---------------+\n",
      "|  stationID|min_temperature|\n",
      "+-----------+---------------+\n",
      "|ITE00100554|         -148.0|\n",
      "|EZE00100082|         -135.0|\n",
      "+-----------+---------------+\n",
      "\n",
      "+-----------+----------------+\n",
      "|  stationID|max(temperature)|\n",
      "+-----------+----------------+\n",
      "|ITE00100554|           323.0|\n",
      "|EZE00100082|           323.0|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"friendsByAge\").getOrCreate()\n",
    "\n",
    "csv_file = \"C:\\\\Users\\\\Amit.Jain\\\\Downloads\\\\learn_spark\\\\1800.csv\"\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"stationID\", StringType(), True), \\\n",
    "                     StructField(\"date\", IntegerType(), True), \\\n",
    "                     StructField(\"measure_type\", StringType(), True), \\\n",
    "                     StructField(\"temperature\", FloatType(), True)])\n",
    "\n",
    "lines = spark.read.option(\"header\",\"false\").option(\"inferSchema\",\"true\").schema(schema).csv(csv_file)\n",
    "#lines.printSchema()\n",
    "#lines.show()\n",
    "\n",
    "minTemps = lines.filter(lines.measure_type==\"TMIN\")\n",
    "maxTemps = lines.filter(lines.measure_type==\"TMAX\")\n",
    "\n",
    "minTemps.groupBy(\"stationID\").min(\"temperature\").alias(\"temperature\").show()\n",
    "minTemps.groupBy(\"stationID\").agg(func.min(\"temperature\").alias(\"min_temperature\")).show()\n",
    "maxTemps.groupBy(\"stationID\").max(\"temperature\").alias(\"temperature\").show()\n",
    "\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0057c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- amount_spent: float (nullable = true)\n",
      "\n",
      "+-------+-------+------------+\n",
      "|cust_id|item_id|amount_spent|\n",
      "+-------+-------+------------+\n",
      "|     44|   8602|       37.19|\n",
      "|     35|   5368|       65.89|\n",
      "|      2|   3391|       40.64|\n",
      "|     47|   6694|       14.98|\n",
      "|     29|    680|       13.08|\n",
      "|     91|   8900|       24.59|\n",
      "|     70|   3959|       68.68|\n",
      "|     85|   1733|       28.53|\n",
      "|     53|   9900|       83.55|\n",
      "|     14|   1505|        4.32|\n",
      "|     51|   3378|        19.8|\n",
      "|     42|   6926|       57.77|\n",
      "|      2|   4424|       55.77|\n",
      "|     79|   9291|       33.17|\n",
      "|     50|   3901|       23.57|\n",
      "|     20|   6633|        6.49|\n",
      "|     15|   6148|       65.53|\n",
      "|     44|   8331|       99.19|\n",
      "|      5|   3505|       64.18|\n",
      "|     48|   5539|       32.42|\n",
      "+-------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-----------+\n",
      "|cust_id|total_spent|\n",
      "+-------+-----------+\n",
      "|     31|    4765.05|\n",
      "|     85|    5503.43|\n",
      "|     65|    5140.35|\n",
      "|     53|     4945.3|\n",
      "|     78|    4524.51|\n",
      "|     34|     5330.8|\n",
      "|     81|    5112.71|\n",
      "|     28|    5000.71|\n",
      "|     76|    4904.21|\n",
      "|     27|    4915.89|\n",
      "|     26|     5250.4|\n",
      "|     44|    4756.89|\n",
      "|     12|    4664.59|\n",
      "|     91|    4642.26|\n",
      "|     22|    5019.45|\n",
      "|     93|    5265.75|\n",
      "|     47|     4316.3|\n",
      "|      1|     4958.6|\n",
      "|     52|    5245.06|\n",
      "|     13|    4367.62|\n",
      "+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"totalSpentByCustomer\").getOrCreate()\n",
    "\n",
    "csv_file = \"C:\\\\Users\\\\Amit.Jain\\\\Downloads\\\\learn_spark\\\\customer-orders.csv\"\n",
    "\n",
    "customerOrderSchema = StructType([ \\\n",
    "                                  StructField(\"cust_id\", IntegerType(), True),\n",
    "                                  StructField(\"item_id\", IntegerType(), True),\n",
    "                                  StructField(\"amount_spent\", FloatType(), True)\n",
    "                                  ])\n",
    "\n",
    "lines = spark.read.option(\"header\",\"false\").option(\"inferSchema\",\"true\").schema(customerOrderSchema).csv(csv_file)\n",
    "lines.printSchema()\n",
    "lines.show()\n",
    "\n",
    "totalByCustomer = lines.groupBy(\"cust_id\").agg(func.round(func.sum(\"amount_spent\"), 2) \\\n",
    "                                      .alias(\"total_spent\"))\n",
    "\n",
    "##lines.groupBy(\"cust_id\").func.sum(\"amount_spent\").show()\n",
    "totalByCustomer.show()         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
